{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf418065-4ecb-4910-81aa-4955c360f51d",
   "metadata": {},
   "source": [
    "## CEPARCO CUDA Project Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c9366e-3b6d-49b7-b4c5-fdad30f92d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/bin:/bin:/usr/bin:/usr/local/cuda/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory containing the executable to the PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/cuda/bin\"\n",
    "\n",
    "# Check if the directory is added to the PATH\n",
    "print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2378b59-8214-4eb6-b077-ba02c2cfd38c",
   "metadata": {},
   "source": [
    "# C Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b7ba44-3988-4cef-8fbf-00724e54758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C_asum.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile C_asum.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h> //fabsf and cos/sin functions\n",
    "\n",
    "void asumfunc(size_t n, double* a, double* asum) {\n",
    "  *asum = 0.0;\n",
    "  for (int i=0; i<n;i++)\n",
    "     *asum += fabs(a[i]);\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv){\n",
    "   const size_t N = 28;\n",
    "   const size_t ARRAY_SIZE = 1<<N;\n",
    "   const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(double);\n",
    "   const size_t loope = 30;\n",
    "\n",
    "   double *a, *asum;\n",
    "   a = (double*)malloc(ARRAY_BYTES);\n",
    "   asum = (double*)malloc(sizeof(double));\n",
    "\n",
    "   clock_t start, end;\n",
    "\n",
    "   for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "    a[i] = sin((double)i * 0.0003) * cos((double)i * 0.0007) * 1000.0;\n",
    "   }\n",
    "\n",
    "   *asum = 0.0;\n",
    "\n",
    "   asumfunc(ARRAY_SIZE,a,asum);\n",
    "\n",
    "   double elapse, time_taken;\n",
    "   elapse = 0.0f;\n",
    "   for (int i=0; i<loope; i++){\n",
    "    start = clock();\n",
    "     asumfunc(ARRAY_SIZE,a,asum);\n",
    "    end = clock();\n",
    "    time_taken = ((double)(end-start))*1E3/CLOCKS_PER_SEC;\n",
    "    elapse = elapse + time_taken;\n",
    "   }\n",
    "\n",
    "    printf(\"Function (in C) average time for %lu loops is %f milliseconds to execute an array size %lu \\n\", loope, elapse/loope, ARRAY_SIZE);\n",
    "    printf(\"Absolute sum of vector size 2^%lu: %lf \\n\",N,*asum);\n",
    "\n",
    "\n",
    "  double err_asum = 0.0;\n",
    "   for (int i=0; i<ARRAY_SIZE; i++)\n",
    "        err_asum += fabs(a[i]);\n",
    "   if (fabs(err_asum - *asum) > 1e-2)\n",
    "        printf(\"Error encountered: \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "   else\n",
    "        printf(\"No errors encountered. \\n (Difference less than 1e-2) \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "\n",
    "    free(a);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcadf0ac-7e29-4e98-b14b-58ff093ff123",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcc C_asum.c -lm -o C_asum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67511fdc-71e9-44f7-938d-b71ba12b4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function (in C) average time for 30 loops is 2421.755667 milliseconds to execute an array size 268435456 \n",
      "Absolute sum of vector size 2^28: 108762865473.985641 \n",
      "No errors encountered. \n",
      " (Difference less than 1e-2) \n",
      " Function result: 108762865473.985641 \n",
      " Error checking result: 108762865473.985641 \n",
      " Error difference: 0.000000 \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./C_asum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08744a15-2db9-4868-8f1d-2d333da9d298",
   "metadata": {},
   "source": [
    "# Grid Stride; no prefetch, no page creation, no mem advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a508ff-83ad-4424-abb9-6f9d8ccee87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_asum1.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_asum1.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h> //fabsf and cos/sin functions\n",
    "\n",
    "__global__\n",
    "void asumfunc(size_t n, double* a, double *asum) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "        atomicAdd(asum,fabs(a[i])); //proper summation function for CUDA\n",
    "}\n",
    "\n",
    "int main(){\n",
    "   const size_t N = 28;\n",
    "   const size_t ARRAY_SIZE = 1<<N;\n",
    "   const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(double);\n",
    "   const size_t loope = 30;\n",
    "\n",
    "   double *a, *asum;\n",
    "   cudaMallocManaged(&a, ARRAY_BYTES);\n",
    "   cudaMallocManaged(&asum, sizeof(double));\n",
    "\n",
    "   for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "    a[i] = sin((double)i * 0.0003) * cos((double)i * 0.0007) * 1000.0;\n",
    "   }\n",
    "\n",
    "   *asum = 0.0;\n",
    "\n",
    "  size_t numThreads = 1024;\n",
    "  size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** function = Double asum\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks,numThreads);\n",
    "  for (size_t i=0; i<loope;i++){\n",
    "    *asum = 0.0;\n",
    "    asumfunc <<<numBlocks, numThreads>>> (ARRAY_SIZE,a,asum);\n",
    "    cudaDeviceSynchronize();\n",
    "  } \n",
    "  //Because the result makes use of a summation function, it is different\n",
    "  //From the CUDA square function where it does not overwrite another array,\n",
    "  //hence each kernel must not overlap their results over another. To fix this,\n",
    "  //cudaDeviceSynchronize() was placed within the loop, as without it *asum = 0.0\n",
    "  //is not properly set and causes the printed result to appear loope times larger.\n",
    "\n",
    "  printf(\"Absolute sum of vector size 2^%lu: %lf \\n\",N,*asum);\n",
    "\n",
    "  //Since summation is performed by each kernel, there is no \"element by element\" error checking\n",
    "  double err_asum = 0.0;\n",
    "   for (int i=0; i<ARRAY_SIZE; i++)\n",
    "        err_asum += fabs(a[i]);\n",
    "   if (fabs(err_asum - *asum) > 1e-2)\n",
    "        printf(\"Error encountered: \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "   else\n",
    "        printf(\"No errors encountered. \\n (Difference less than 1e-2) \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "\n",
    "  cudaFree(a);\n",
    "  cudaFree(asum);\n",
    "  return 0;\n",
    "}\n",
    "//In compiling, -arch=sm_60 allows CUDA 6.0 for use of atomicAdd with double* and double parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81728ddc-38ce-47ec-bad4-fad706ad97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_asum1.cu -lm -o CUDA_asum1 -Wno-deprecated-gpu-targets -arch=sm_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc33bb5d-c763-4cec-b063-185d98238bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008037== NVPROF is profiling process 1008037, command: ./CUDA_asum1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** function = Double asum\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Absolute sum of vector size 2^28: 108762865473.981369 \n",
      "No errors encountered. \n",
      " (Difference less than 1e-2) \n",
      " Function result: 108762865473.981369 \n",
      " Error checking result: 108762865473.985641 \n",
      " Error difference: 0.004272 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008037== Profiling application: ./CUDA_asum1\n",
      "==1008037== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  20.1592s        30  671.97ms  614.79ms  1.98873s  asumfunc(unsigned long, double*, double*)\n",
      "      API calls:   89.87%  20.1620s        30  672.07ms  614.92ms  1.98889s  cudaDeviceSynchronize\n",
      "                    9.07%  2.03515s         2  1.01758s  1.1067ms  2.03405s  cudaMallocManaged\n",
      "                    0.67%  151.41ms         2  75.703ms  698.66us  150.71ms  cudaFree\n",
      "                    0.36%  81.105ms        30  2.7035ms  259.39us  67.165ms  cudaLaunchKernel\n",
      "                    0.02%  4.1609ms         1  4.1609ms  4.1609ms  4.1609ms  cuDeviceTotalMem\n",
      "                    0.00%  863.91us       114  7.5780us     106ns  404.52us  cuDeviceGetAttribute\n",
      "                    0.00%  251.68us         1  251.68us  251.68us  251.68us  cuDeviceGetName\n",
      "                    0.00%  44.340us         1  44.340us  44.340us  44.340us  cuDeviceGetPCIBusId\n",
      "                    0.00%  9.2980us         3  3.0990us     131ns  7.6290us  cuDeviceGetCount\n",
      "                    0.00%  3.3280us         2  1.6640us     144ns  3.1840us  cuDeviceGet\n",
      "                    0.00%  1.1630us         1  1.1630us  1.1630us  1.1630us  cuModuleGetLoadingMode\n",
      "                    0.00%     969ns         1     969ns     969ns     969ns  cuDeviceGetUuid\n",
      "\n",
      "==1008037== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "   24965  38.040KB  4.0000KB  0.9961MB  927.4180MB  258.4782ms  Host To Device\n",
      "   12346  170.02KB  4.0000KB  0.9961MB  2.001770GB  841.8155ms  Device To Host\n",
      "    1919         -         -         -           -   1.051416s  Gpu page fault groups\n",
      "Total CPU Page faults: 12319\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_asum1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3cbc5-b6d7-4917-a5c6-faa9692b6636",
   "metadata": {},
   "source": [
    "# Grid Stride; with prefetch, no page creation, no mem advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9950c74c-76bb-4501-b098-da9f1d572d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_asum2.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_asum2.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void asumfunc(size_t n, double* a, double *asum) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "        atomicAdd(asum,fabs(a[i]));\n",
    "}\n",
    "\n",
    "\n",
    "int main(){\n",
    "   const size_t N = 28;\n",
    "   const size_t ARRAY_SIZE = 1<<N;\n",
    "   const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(double);\n",
    "   const size_t loope = 30;\n",
    "\n",
    "   double *a, *asum;\n",
    "   cudaMallocManaged(&a, ARRAY_BYTES);\n",
    "   cudaMallocManaged(&asum, sizeof(double));\n",
    "\n",
    "  int device = -1;\n",
    "  cudaGetDevice(&device);\n",
    "\n",
    "   for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "    a[i] = sin((double)i * 0.0003) * cos((double)i * 0.0007) * 1000.0;\n",
    "   }\n",
    "\n",
    "   *asum = 0.0;\n",
    "\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,device,NULL);\n",
    "\n",
    "  size_t numThreads = 1024;\n",
    "  size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** function = Double asum\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks,numThreads);\n",
    "  for (size_t i=0; i<loope;i++){\n",
    "    *asum = 0.0;\n",
    "    asumfunc <<<numBlocks, numThreads>>> (ARRAY_SIZE,a,asum);\n",
    "    cudaDeviceSynchronize();\n",
    "  }\n",
    "\n",
    "//\"Prefetch data\" from GPU-CPU\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(asum,sizeof(double),cudaCpuDeviceId,NULL);\n",
    "\n",
    "  printf(\"Absolute sum of vector size 2^%lu: %lf \\n\",N,*asum);\n",
    "\n",
    "  double err_asum = 0.0;\n",
    "   for (int i=0; i<ARRAY_SIZE; i++)\n",
    "        err_asum += fabs(a[i]);\n",
    "   if (fabs(err_asum - *asum) > 1e-2)\n",
    "        printf(\"Error encountered: \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "   else\n",
    "        printf(\"No errors encountered. \\n (Difference less than 1e-2) \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "\n",
    "  cudaFree(a);\n",
    "  cudaFree(asum);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182b8941-d13e-49a8-b915-9acc11fb4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_asum2.cu -lm -o CUDA_asum2 -Wno-deprecated-gpu-targets -arch=sm_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9111de01-ebbf-4a8b-9e65-59dfffc9b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008576== NVPROF is profiling process 1008576, command: ./CUDA_asum2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** function = Double asum\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Absolute sum of vector size 2^28: 108762865473.984619 \n",
      "No errors encountered. \n",
      " (Difference less than 1e-2) \n",
      " Function result: 108762865473.984619 \n",
      " Error checking result: 108762865473.985641 \n",
      " Error difference: 0.001022 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008576== Profiling application: ./CUDA_asum2\n",
      "==1008576== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  18.4628s        30  615.43ms  614.75ms  620.26ms  asumfunc(unsigned long, double*, double*)\n",
      "      API calls:   86.97%  18.4658s        30  615.53ms  614.74ms  620.28ms  cudaDeviceSynchronize\n",
      "                    6.70%  1.42369s         2  711.84ms  1.0738ms  1.42261s  cudaMallocManaged\n",
      "                    5.05%  1.07144s         3  357.15ms  1.0313ms  848.08ms  cudaMemPrefetchAsync\n",
      "                    0.78%  165.36ms         2  82.678ms  1.1198ms  164.24ms  cudaFree\n",
      "                    0.50%  105.93ms        30  3.5311ms  258.33us  92.310ms  cudaLaunchKernel\n",
      "                    0.00%  680.75us       114  5.9710us     140ns  297.30us  cuDeviceGetAttribute\n",
      "                    0.00%  367.64us         1  367.64us  367.64us  367.64us  cuDeviceGetName\n",
      "                    0.00%  46.289us         1  46.289us  46.289us  46.289us  cuDeviceTotalMem\n",
      "                    0.00%  18.516us         1  18.516us  18.516us  18.516us  cudaGetDevice\n",
      "                    0.00%  17.718us         1  17.718us  17.718us  17.718us  cuDeviceGetPCIBusId\n",
      "                    0.00%  11.786us         3  3.9280us     229ns  6.1720us  cuDeviceGetCount\n",
      "                    0.00%  10.775us         2  5.3870us  1.7440us  9.0310us  cuDeviceGet\n",
      "                    0.00%  2.0320us         1  2.0320us  2.0320us  2.0320us  cuModuleGetLoadingMode\n",
      "                    0.00%  1.6740us         1  1.6740us  1.6740us  1.6740us  cuDeviceGetUuid\n",
      "\n",
      "==1008576== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    1084  1.8910MB  4.0000KB  2.0000MB  2.001831GB  310.5582ms  Host To Device\n",
      "    1083  1.8927MB  4.0000KB  2.0000MB  2.001774GB  834.9982ms  Device To Host\n",
      "      30         -         -         -           -  49.06896ms  Gpu page fault groups\n",
      "Total CPU Page faults: 6174\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_asum2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb31e03-22ba-4797-aea8-1fe62a25f3d2",
   "metadata": {},
   "source": [
    "# Grid Stride; with prefetch; with page creation; no mem advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb8fdb9-ea82-4fb9-b58e-89bdc5152619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_asum3.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_asum3.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void asumfunc(size_t n, double* a, double *asum) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "        atomicAdd(asum,fabs(a[i]));\n",
    "}\n",
    "\n",
    "\n",
    "int main(){\n",
    "   const size_t N = 28;\n",
    "   const size_t ARRAY_SIZE = 1<<N;\n",
    "   const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(double);\n",
    "   const size_t loope = 30;\n",
    "\n",
    "   double *a, *asum;\n",
    "   cudaMallocManaged(&a, ARRAY_BYTES);\n",
    "   cudaMallocManaged(&asum, sizeof(double));\n",
    "\n",
    "  int device = -1;\n",
    "  cudaGetDevice(&device);\n",
    "\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(asum,sizeof(double),device,NULL);\n",
    "\n",
    "   for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "    a[i] = sin((double)i * 0.0003) * cos((double)i * 0.0007) * 1000.0;\n",
    "   }\n",
    "\n",
    "   *asum = 0.0;\n",
    "\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,device,NULL);\n",
    "\n",
    "  size_t numThreads = 1024;\n",
    "  size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** function = Double asum\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks,numThreads);\n",
    "  for (size_t i=0; i<loope;i++){\n",
    "    *asum = 0.0;\n",
    "    asumfunc <<<numBlocks, numThreads>>> (ARRAY_SIZE,a,asum);\n",
    "    cudaDeviceSynchronize();\n",
    "  }\n",
    "\n",
    "//\"Prefetch data\" from GPU-CPU\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(asum,sizeof(double),cudaCpuDeviceId,NULL);\n",
    "\n",
    "  printf(\"Absolute sum of vector size 2^%lu: %lf \\n\",N,*asum);\n",
    "\n",
    "  double err_asum = 0.0;\n",
    "   for (int i=0; i<ARRAY_SIZE; i++)\n",
    "        err_asum += fabs(a[i]);\n",
    "   if (fabs(err_asum - *asum) > 1e-2)\n",
    "        printf(\"Error encountered: \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "   else\n",
    "        printf(\"No errors encountered. \\n (Difference less than 1e-2) \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "\n",
    "  cudaFree(a);\n",
    "  cudaFree(asum);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e3359d-67eb-4f21-b211-1c1b99325a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_asum3.cu -lm -o CUDA_asum3 -Wno-deprecated-gpu-targets -arch=sm_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7cc9228-f50b-49be-9895-887de9de80b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008802== NVPROF is profiling process 1008802, command: ./CUDA_asum3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** function = Double asum\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Absolute sum of vector size 2^28: 108762865473.983490 \n",
      "No errors encountered. \n",
      " (Difference less than 1e-2) \n",
      " Function result: 108762865473.983490 \n",
      " Error checking result: 108762865473.985641 \n",
      " Error difference: 0.002151 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008802== Profiling application: ./CUDA_asum3\n",
      "==1008802== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  18.4575s        30  615.25ms  614.57ms  616.13ms  asumfunc(unsigned long, double*, double*)\n",
      "      API calls:   80.28%  18.4593s        30  615.31ms  614.68ms  616.18ms  cudaDeviceSynchronize\n",
      "                   12.18%  2.80118s         5  560.24ms  760.33us  1.76196s  cudaMemPrefetchAsync\n",
      "                    6.05%  1.39158s         2  695.79ms  1.2224ms  1.39036s  cudaMallocManaged\n",
      "                    0.79%  182.80ms        30  6.0933ms  153.60us  172.27ms  cudaLaunchKernel\n",
      "                    0.69%  158.36ms         2  79.179ms  1.7866ms  156.57ms  cudaFree\n",
      "                    0.00%  638.15us       114  5.5970us     110ns  315.59us  cuDeviceGetAttribute\n",
      "                    0.00%  183.88us         1  183.88us  183.88us  183.88us  cuDeviceGetName\n",
      "                    0.00%  29.022us         1  29.022us  29.022us  29.022us  cuDeviceTotalMem\n",
      "                    0.00%  20.974us         1  20.974us  20.974us  20.974us  cudaGetDevice\n",
      "                    0.00%  14.842us         1  14.842us  14.842us  14.842us  cuDeviceGetPCIBusId\n",
      "                    0.00%  8.1600us         3  2.7200us     120ns  7.4900us  cuDeviceGetCount\n",
      "                    0.00%  5.2410us         2  2.6200us  1.4520us  3.7890us  cuDeviceGet\n",
      "                    0.00%  1.2080us         1  1.2080us  1.2080us  1.2080us  cuModuleGetLoadingMode\n",
      "                    0.00%     572ns         1     572ns     572ns     572ns  cuDeviceGetUuid\n",
      "\n",
      "==1008802== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    1084  1.8910MB  4.0000KB  2.0000MB  2.001831GB  308.5915ms  Host To Device\n",
      "    1084  1.8910MB  4.0000KB  2.0000MB  2.001778GB  882.1062ms  Device To Host\n",
      "      30         -         -         -           -  43.59782ms  Gpu page fault groups\n",
      "Total CPU Page faults: 30\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_asum3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400a302-948c-4fb3-9176-69e7f3b81fb2",
   "metadata": {},
   "source": [
    "# Grid Stride; with prefetch; with page creation; with mem advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac10977b-3b43-401b-8557-10001a78e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_asum4.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_asum4.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void asumfunc(size_t n, double* a, double *asum) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "        atomicAdd(asum,fabs(a[i]));\n",
    "}\n",
    "\n",
    "\n",
    "int main(){\n",
    "   const size_t N = 28;\n",
    "   const size_t ARRAY_SIZE = 1<<N;\n",
    "   const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(double);\n",
    "   const size_t loope = 30;\n",
    "\n",
    "   double *a, *asum;\n",
    "   cudaMallocManaged(&a, ARRAY_BYTES);\n",
    "   cudaMallocManaged(&asum, sizeof(double));\n",
    "\n",
    "  int device = -1;\n",
    "  cudaGetDevice(&device);\n",
    "\n",
    "\n",
    "// memory advise\n",
    "   cudaMemAdvise(a, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
    "   cudaMemAdvise(a, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
    "\n",
    "//\"prefetch data\" to create CPU page memory\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "//\"prefetch data\" to create GPU page memory\n",
    "  cudaMemPrefetchAsync(asum,sizeof(double),device,NULL);\n",
    "\n",
    "// ****init array\n",
    "   for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "    a[i] = sin((double)i * 0.0003) * cos((double)i * 0.0007) * 1000.0;\n",
    "   }\n",
    "\n",
    "   *asum = 0.0;\n",
    "\n",
    " //\"Prefetch data\" from CPU-GPU\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,device,NULL);\n",
    "\n",
    "// setup CUDA kernel\n",
    "  size_t numThreads = 1024;\n",
    "  size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** function = Double asum\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks,numThreads);\n",
    "  for (size_t i=0; i<loope;i++){\n",
    "    *asum = 0.0;\n",
    "    asumfunc <<<numBlocks, numThreads>>> (ARRAY_SIZE,a,asum);\n",
    "    cudaDeviceSynchronize();\n",
    "  }\n",
    "\n",
    "//\"Prefetch data\" from GPU-CPU\n",
    "  cudaMemPrefetchAsync(a,ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(asum,sizeof(double),cudaCpuDeviceId,NULL);\n",
    "\n",
    "  printf(\"Absolute sum of vector size 2^%lu: %lf \\n\",N,*asum);\n",
    "\n",
    "  double err_asum = 0.0;\n",
    "   for (int i=0; i<ARRAY_SIZE; i++)\n",
    "        err_asum += fabs(a[i]);\n",
    "   if (fabs(err_asum - *asum) > 1e-2)\n",
    "        printf(\"Error encountered: \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "   else\n",
    "        printf(\"No errors encountered. \\n (Difference less than 1e-2) \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "\n",
    "  cudaFree(a);\n",
    "  cudaFree(asum);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dc45ca7-6eb9-4207-8cb3-45cd076f4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_asum4.cu -lm -o CUDA_asum4 -Wno-deprecated-gpu-targets -arch=sm_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c6ebb71-1983-4772-bd2c-16173ad5a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008877== NVPROF is profiling process 1008877, command: ./CUDA_asum4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** function = Double asum\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Absolute sum of vector size 2^28: 108762865473.985596 \n",
      "No errors encountered. \n",
      " (Difference less than 1e-2) \n",
      " Function result: 108762865473.985596 \n",
      " Error checking result: 108762865473.985641 \n",
      " Error difference: 0.000046 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008877== Profiling application: ./CUDA_asum4\n",
      "==1008877== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  18.4556s        30  615.19ms  614.61ms  616.76ms  asumfunc(unsigned long, double*, double*)\n",
      "      API calls:   83.16%  18.4579s        30  615.26ms  614.69ms  616.82ms  cudaDeviceSynchronize\n",
      "                    9.72%  2.15840s         5  431.68ms  411.37us  1.83960s  cudaMemPrefetchAsync\n",
      "                    6.24%  1.38593s         2  692.97ms  695.74us  1.38524s  cudaMallocManaged\n",
      "                    0.80%  178.37ms         2  89.183ms  9.9370ms  168.43ms  cudaFree\n",
      "                    0.06%  12.965ms        30  432.15us  196.07us  2.1501ms  cudaLaunchKernel\n",
      "                    0.00%  449.89us       114  3.9460us     129ns  173.92us  cuDeviceGetAttribute\n",
      "                    0.00%  214.44us         1  214.44us  214.44us  214.44us  cuDeviceGetName\n",
      "                    0.00%  134.74us         2  67.369us  10.152us  124.59us  cudaMemAdvise\n",
      "                    0.00%  69.478us         1  69.478us  69.478us  69.478us  cudaGetDevice\n",
      "                    0.00%  23.915us         1  23.915us  23.915us  23.915us  cuDeviceGetPCIBusId\n",
      "                    0.00%  22.621us         1  22.621us  22.621us  22.621us  cuDeviceTotalMem\n",
      "                    0.00%  8.9170us         3  2.9720us     167ns  8.5090us  cuDeviceGetCount\n",
      "                    0.00%  4.2130us         2  2.1060us     156ns  4.0570us  cuDeviceGet\n",
      "                    0.00%     741ns         1     741ns     741ns     741ns  cuModuleGetLoadingMode\n",
      "                    0.00%     698ns         1     698ns     698ns     698ns  cuDeviceGetUuid\n",
      "\n",
      "==1008877== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    1084  1.8910MB  4.0000KB  2.0000MB  2.001831GB  292.3934ms  Host To Device\n",
      "      60  31.066KB  4.0000KB  60.000KB  1.820313MB  481.8540us  Device To Host\n",
      "      30         -         -         -           -  41.56272ms  Gpu page fault groups\n",
      "Total CPU Page faults: 30\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_asum4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be1478-ff4f-4856-bea0-8ce50bb5935c",
   "metadata": {},
   "source": [
    "# Classic MemCopy (no Unified Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46e91650-f6bd-4f36-9bf3-67cc8175b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_asum5.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_asum5.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <time.h>\n",
    "\n",
    "__global__\n",
    "void asumfunc(size_t n, double* a, double *asum) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    double localSum = 0.0;\n",
    "\n",
    "    // Each thread sums part of the array\n",
    "    for (int i = index; i < n; i += stride)\n",
    "        localSum += fabs(a[i]);\n",
    "\n",
    "    // Atomic add partial results to global sum\n",
    "    atomicAdd(asum, localSum);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const size_t N = 28;\n",
    "    const size_t ARRAY_SIZE = 1 << N;\n",
    "    const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(double);\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    // Allocate host memory (regular malloc)\n",
    "    double *h_a = (double*)malloc(ARRAY_BYTES);\n",
    "    double h_asum = 0.0;\n",
    "\n",
    "    // Initialize host data\n",
    "    for (size_t i = 0; i < ARRAY_SIZE; i++)\n",
    "        h_a[i] = sin((double)i * 0.0003) * cos((double)i * 0.0007) * 1000.0;\n",
    "\n",
    "    // Allocate device memory (no Unified Memory)\n",
    "    double *d_a, *d_asum;\n",
    "    cudaMalloc(&d_a, ARRAY_BYTES);\n",
    "    cudaMalloc(&d_asum, sizeof(double));\n",
    "\n",
    "    // Copy data to device\n",
    "    cudaMemcpy(d_a, h_a, ARRAY_BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_asum, &h_asum, sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Kernel setup\n",
    "    size_t numThreads = 1024;\n",
    "    size_t numBlocks = (ARRAY_SIZE + numThreads - 1) / numThreads;\n",
    "\n",
    "    printf(\"*** function = Double asum (Classic MemCopy)\\n\");\n",
    "    printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu\\n\", numBlocks, numThreads);\n",
    "\n",
    "    // Run kernel multiple times\n",
    "    double zero = 0.0;\n",
    "    for (size_t i = 0; i < loope; i++) {\n",
    "        cudaMemcpy(d_asum, &zero, sizeof(double), cudaMemcpyHostToDevice);\n",
    "        asumfunc<<<numBlocks, numThreads>>>(ARRAY_SIZE, d_a, d_asum);\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(&h_asum, d_asum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    printf(\"Absolute sum of vector size 2^%lu: %lf\\n\", N, h_asum);\n",
    "\n",
    "    // Validate result\n",
    "    double err_asum = 0.0;\n",
    "    for (size_t i = 0; i < ARRAY_SIZE; i++)\n",
    "        err_asum += fabs(h_a[i]);\n",
    "\n",
    "    if (fabs(err_asum - h_asum) > 1e-2)\n",
    "        printf(\"Error encountered: \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", h_asum, err_asum, (err_asum - h_asum));\n",
    "    else\n",
    "        printf(\"No errors encountered. \\n (Difference less than 1e-2) \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", h_asum, err_asum, (err_asum - h_asum));\n",
    "\n",
    "    // Free memory\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_asum);\n",
    "    free(h_a);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d33c269-12cf-4fa4-89e0-22b9e87bc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_asum5.cu -lm -o CUDA_asum5 -Wno-deprecated-gpu-targets -arch=sm_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "457efc26-6dda-43f3-b98c-68d37c370d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008993== NVPROF is profiling process 1008993, command: ./CUDA_asum5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** function = Double asum (Classic MemCopy)\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024\n",
      "Absolute sum of vector size 2^28: 108762865473.984940\n",
      "No errors encountered. \n",
      " (Difference less than 1e-2) \n",
      " Function result: 108762865473.984940 \n",
      " Error checking result: 108762865473.985641 \n",
      " Error difference: 0.000702 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1008993== Profiling application: ./CUDA_asum5\n",
      "==1008993== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   89.37%  18.4266s        30  614.22ms  613.67ms  627.89ms  asumfunc(unsigned long, double*, double*)\n",
      "                   10.63%  2.19124s        32  68.476ms     640ns  2.19122s  [CUDA memcpy HtoD]\n",
      "                    0.00%  3.4880us         1  3.4880us  3.4880us  3.4880us  [CUDA memcpy DtoH]\n",
      "      API calls:   83.95%  18.4294s        30  614.31ms  613.74ms  627.96ms  cudaDeviceSynchronize\n",
      "                   10.02%  2.20024s        33  66.674ms  47.516us  2.19254s  cudaMemcpy\n",
      "                    5.77%  1.26690s         2  633.45ms  329.57us  1.26657s  cudaMalloc\n",
      "                    0.23%  49.851ms        30  1.6617ms  121.21us  43.886ms  cudaLaunchKernel\n",
      "                    0.03%  5.8133ms         2  2.9066ms  1.0135ms  4.7998ms  cudaFree\n",
      "                    0.00%  641.33us       114  5.6250us     106ns  251.94us  cuDeviceGetAttribute\n",
      "                    0.00%  244.86us         1  244.86us  244.86us  244.86us  cuDeviceGetName\n",
      "                    0.00%  54.799us         1  54.799us  54.799us  54.799us  cuDeviceTotalMem\n",
      "                    0.00%  18.063us         1  18.063us  18.063us  18.063us  cuDeviceGetPCIBusId\n",
      "                    0.00%  4.6280us         3  1.5420us     173ns  4.2410us  cuDeviceGetCount\n",
      "                    0.00%  3.5930us         2  1.7960us  1.2670us  2.3260us  cuDeviceGet\n",
      "                    0.00%  1.6810us         1  1.6810us  1.6810us  1.6810us  cuModuleGetLoadingMode\n",
      "                    0.00%  1.6020us         1  1.6020us  1.6020us  1.6020us  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_asum5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e9143-484d-4af0-801d-9e1a38723006",
   "metadata": {},
   "source": [
    "# Grid-Stride Loop with Prefetch and GPU Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0e100d-4320-4cc7-b776-1dcdce4de7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_asum6.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_asum6.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void init_array(size_t n, double* a) {\n",
    "    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = blockDim.x * gridDim.x;\n",
    "    for (size_t i = index; i < n; i += stride) {\n",
    "        a[i] = sin((double)i * 0.0003) * cos((double)i * 0.0007) * 1000.0;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__\n",
    "void asumfunc(size_t n, double* a, double *asum) {\n",
    "    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = blockDim.x * gridDim.x;\n",
    "    for (size_t i = index; i < n; i += stride)\n",
    "        atomicAdd(asum, fabs(a[i]));\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const size_t N = 28;\n",
    "    const size_t ARRAY_SIZE = 1 << N;\n",
    "    const size_t ARRAY_BYTES = ARRAY_SIZE * sizeof(double);\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    double *a, *asum;\n",
    "    cudaMallocManaged(&a, ARRAY_BYTES);\n",
    "    cudaMallocManaged(&asum, sizeof(double));\n",
    "\n",
    "    int device = -1;\n",
    "    cudaGetDevice(&device);\n",
    "\n",
    "    // Prefetch to GPU before initializing\n",
    "    cudaMemPrefetchAsync(a, ARRAY_BYTES, device, NULL);\n",
    "    cudaMemPrefetchAsync(asum, sizeof(double), device, NULL);\n",
    "\n",
    "    size_t numThreads = 1024;\n",
    "    size_t numBlocks = (ARRAY_SIZE + numThreads - 1) / numThreads;\n",
    "\n",
    "    // GPU kernel to initialize array\n",
    "    init_array<<<numBlocks, numThreads>>>(ARRAY_SIZE, a);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    *asum = 0.0;\n",
    "\n",
    "    printf(\"*** function = Double asum (GPU init data)\\n\");\n",
    "    printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\", numBlocks, numThreads);\n",
    "\n",
    "    for (size_t i=0; i<loope;i++){\n",
    "      *asum = 0.0;\n",
    "      asumfunc <<<numBlocks, numThreads>>> (ARRAY_SIZE,a,asum);\n",
    "      cudaDeviceSynchronize();\n",
    "    }\n",
    "\n",
    "    // Prefetch back to CPU for validation\n",
    "    cudaMemPrefetchAsync(a, ARRAY_BYTES, cudaCpuDeviceId, NULL);\n",
    "    cudaMemPrefetchAsync(asum, sizeof(double), cudaCpuDeviceId, NULL);\n",
    "\n",
    "    printf(\"Absolute sum of vector size 2^%lu: %lf \\n\", N, *asum);\n",
    "\n",
    "    // Verify result on CPU\n",
    "    double err_asum = 0.0;\n",
    "    for (size_t i = 0; i < ARRAY_SIZE; i++)\n",
    "        err_asum += fabs(a[i]);\n",
    "\n",
    "    if (fabs(err_asum - *asum) > 1e-2)\n",
    "        printf(\"Error encountered: \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "    else\n",
    "        printf(\"No errors encountered. \\n (Difference less than 1e-2) \\n Function result: %lf \\n Error checking result: %lf \\n Error difference: %lf \\n\", *asum, err_asum, (err_asum - *asum));\n",
    "\n",
    "    cudaFree(a);\n",
    "    cudaFree(asum);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf2f58f-f1cd-4de4-9aba-7c91c1e92b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_asum6.cu -lm -o CUDA_asum6 -Wno-deprecated-gpu-targets -arch=sm_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80c7da97-9001-4eca-b85e-48821e5041b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1009060== NVPROF is profiling process 1009060, command: ./CUDA_asum6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** function = Double asum (GPU init data)\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Absolute sum of vector size 2^28: 108762865473.984177 \n",
      "No errors encountered. \n",
      " (Difference less than 1e-2) \n",
      " Function result: 108762865473.984177 \n",
      " Error checking result: 108762865473.985641 \n",
      " Error difference: 0.001465 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1009060== Profiling application: ./CUDA_asum6\n",
      "==1009060== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   99.98%  18.4964s        30  616.55ms  614.59ms  641.18ms  asumfunc(unsigned long, double*, double*)\n",
      "                    0.02%  3.6316ms         1  3.6316ms  3.6316ms  3.6316ms  init_array(unsigned long, double*)\n",
      "      API calls:   85.35%  18.5016s        31  596.83ms  3.6929ms  641.25ms  cudaDeviceSynchronize\n",
      "                    7.34%  1.59110s         4  397.78ms  591.59us  1.56934s  cudaMemPrefetchAsync\n",
      "                    6.13%  1.32845s         2  664.23ms  1.1223ms  1.32733s  cudaMallocManaged\n",
      "                    0.66%  143.21ms         2  71.604ms  1.2653ms  141.94ms  cudaFree\n",
      "                    0.52%  112.34ms        31  3.6239ms  194.94us  98.757ms  cudaLaunchKernel\n",
      "                    0.00%  1.0763ms       114  9.4410us     103ns  726.11us  cuDeviceGetAttribute\n",
      "                    0.00%  211.62us         1  211.62us  211.62us  211.62us  cuDeviceTotalMem\n",
      "                    0.00%  146.49us         1  146.49us  146.49us  146.49us  cuDeviceGetName\n",
      "                    0.00%  26.538us         1  26.538us  26.538us  26.538us  cudaGetDevice\n",
      "                    0.00%  12.982us         3  4.3270us     268ns  12.404us  cuDeviceGetCount\n",
      "                    0.00%  12.637us         1  12.637us  12.637us  12.637us  cuDeviceGetPCIBusId\n",
      "                    0.00%  5.9850us         2  2.9920us     767ns  5.2180us  cuDeviceGet\n",
      "                    0.00%     759ns         1     759ns     759ns     759ns  cuDeviceGetUuid\n",
      "                    0.00%     608ns         1     608ns     608ns     608ns  cuModuleGetLoadingMode\n",
      "\n",
      "==1009060== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "      60  32.000KB  4.0000KB  60.000KB  1.875000MB  503.5430us  Host To Device\n",
      "    1084  1.8910MB  4.0000KB  2.0000MB  2.001778GB   1.151475s  Device To Host\n",
      "      30         -         -         -           -  62.45958ms  Gpu page fault groups\n",
      "Total CPU Page faults: 30\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_asum6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ca515-4ce5-4933-9b10-fcad4130013c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708502b-8377-42f9-9c84-db20d5bafca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
